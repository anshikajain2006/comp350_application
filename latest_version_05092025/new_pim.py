from __future__ import annotations
from fastapi import FastAPI, Request, Response, HTTPException, status, Cookie, Depends
from fastapi.responses import FileResponse, HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field
from dataclasses import dataclass, asdict
from typing import Optional, Iterable, Annotated, Tuple, List
import os
import hmac
import uuid
import json
import secrets
import hashlib
import sqlite3
from datetime import datetime, timedelta

import new_pim as pim

# -*- coding: utf-8 -*-
"""04-09-pim

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dKuKUBaHh3wfuyNUG686APWQyA4XtQ76
"""

# auth_app.py
# ------------------------------------------------------------
# Secure auth with:
# - salted PBKDF2 password hashing
# - HMAC-signed cookie sessions (your code block, integrated)
# - SQLite session store
# - exponential backoff on failed logins
# - double-submit-cookie CSRF protection for state-changing routes
# - preserves function names: login, logout, create_new_user
# ------------------------------------------------------------
# -------------------- DB SETUP --------------------
con = sqlite3.connect("pim.db", check_same_thread=False)
con.execute("PRAGMA foreign_keys = ON")

con.execute("""
CREATE TABLE IF NOT EXISTS Users(
  user_id INTEGER PRIMARY KEY AUTOINCREMENT,
  username TEXT UNIQUE,
  -- legacy plaintext column (kept temporarily):
  password TEXT,
  password_salt TEXT,
  password_hash TEXT
);
""")

con.execute("""
CREATE TABLE IF NOT EXISTS Particles(
  particle_id INTEGER PRIMARY KEY AUTOINCREMENT,
  date_created TEXT,
  date_updated TEXT,
  title TEXT,
  body TEXT,
  tags TEXT,
  particle_references TEXT
);
""")

con.execute("""
CREATE TABLE IF NOT EXISTS Sessions(
  session_token TEXT PRIMARY KEY,         -- uuid4 string (we'll store full cookie 'sessionid')
  signature TEXT NOT NULL,                -- HMAC signature for the sessionid
  user_id INTEGER NOT NULL,
  created_at TEXT NOT NULL,
  expires_at TEXT NOT NULL,
  user_agent TEXT,
  ip TEXT,
  FOREIGN KEY(user_id) REFERENCES Users(user_id) ON DELETE CASCADE
);
""")

con.execute("""
CREATE TABLE IF NOT EXISTS FailedLogins(
  username TEXT PRIMARY KEY,
  fail_count INTEGER NOT NULL DEFAULT 0,
  last_failed_at TEXT
);
""")

con.commit()

# -------------------- MODELS --------------------
@dataclass
class User:
  user_id: Optional[int]
  username: str
  password: str       # compatibility only; not used
  token: Optional[str]

@dataclass
class Particle:
  particle_id: Optional[int]
  date_created: datetime
  date_updated: datetime
  title: str
  body: str
  tags: list
  particle_references: list

# -------------------- PASSWORDS --------------------
_PBKDF_ALGO = "sha256"
_PBKDF_ITER = 200_000
_SALT_BYTES = 32
_HASH_BYTES = 32

def _hex(b: bytes) -> str: return b.hex()
def _unhex(s: str) -> bytes: return bytes.fromhex(s or "")

def hash_password(plain: str) -> tuple[str, str]:
    salt = os.urandom(_SALT_BYTES)
    digest = hashlib.pbkdf2_hmac(_PBKDF_ALGO, plain.encode(), salt, _PBKDF_ITER, dklen=_HASH_BYTES)
    return _hex(salt), _hex(digest)

def verify_password(plain: str, salt_hex: str, hash_hex: str) -> bool:
    if not salt_hex or not hash_hex: return False
    test = hashlib.pbkdf2_hmac(_PBKDF_ALGO, plain.encode(), _unhex(salt_hex), _PBKDF_ITER, dklen=len(_unhex(hash_hex)))
    return hmac.compare_digest(test, _unhex(hash_hex))

# -------------------- RATE LIMIT / BACKOFF --------------------
def _login_backoff_seconds(username: str) -> int:
    row = con.execute("SELECT fail_count, last_failed_at FROM FailedLogins WHERE username = ?", (username,)).fetchone()
    if not row: return 0
    fails, last = row
    if not last: return 0
    last_dt = datetime.fromisoformat(last)
    # Start backoff after 3 failures: 1, 2, 4, 8, … minutes (cap 30 min)
    if fails <= 3: return 0
    minutes = min(2 ** (fails - 3), 30)
    unlock_time = last_dt + timedelta(minutes=minutes)
    delta = (unlock_time - datetime.utcnow()).total_seconds()
    return max(0, int(delta))

def _record_login_failure(username: str) -> None:
    row = con.execute("SELECT fail_count FROM FailedLogins WHERE username = ?", (username,)).fetchone()
    now = datetime.utcnow().isoformat()
    if row:
        con.execute("UPDATE FailedLogins SET fail_count = fail_count + 1, last_failed_at = ? WHERE username = ?", (now, username))
    else:
        con.execute("INSERT INTO FailedLogins(username, fail_count, last_failed_at) VALUES (?, ?, ?)", (username, 1, now))
    con.commit()

def _reset_login_failures(username: str) -> None:
    con.execute("DELETE FROM FailedLogins WHERE username = ?", (username,))
    con.commit()

# -------------------- CSRf (double submit cookie) --------------------
CSRF_COOKIE = "csrf"
def set_csrf_cookie(resp: Response) -> str:
    token = secrets.token_urlsafe(24)
    resp.set_cookie(
        key=CSRF_COOKIE,
        value=token,
        httponly=False,     # must be readable by JS to attach to header; still safe via same-site
        secure=True,
        samesite="lax",
        max_age=60 * 60 * 24 * 7,
        path="/"
    )
    return token

def csrf_protect(request: Request, csrf_cookie: Annotated[Optional[str], Cookie(alias=CSRF_COOKIE)] = None):
    # For state-changing methods, require header to match cookie
    if request.method in ("POST", "PUT", "PATCH", "DELETE"):
        header = request.headers.get("x-csrf-token")
        if not csrf_cookie or not header or not hmac.compare_digest(header, csrf_cookie):
            raise HTTPException(status_code=403, detail="CSRF validation failed")

# -------------------- SESSION SIGNING (YOUR CODE BLOCK) --------------------
class Credentials(BaseModel):
    username: str
    password: str

server_secret_key = secrets.token_bytes(nbytes=32)
# This key is usually managed by an internal service and
# secure store, but is ok to generate fresh on start of
# server. Ideally it will also have to be "rotated"
# (i.e. changed) at some interval.

def sign(text: str):
    return hmac.digest(server_secret_key, text.encode(), 'sha256').hex()

# -------------------- PUBLIC API (keep names) --------------------
def create_new_user(name: str, pw: str) -> User:
    salt_hex, hash_hex = hash_password(pw)
    con.execute(
        "INSERT INTO Users(username, password, password_salt, password_hash) VALUES (?, ?, ?, ?)",
        (name, None, salt_hex, hash_hex)
    )
    con.commit()
    user_id = con.execute("SELECT user_id FROM Users WHERE username = ?", (name,)).fetchone()[0]
    return User(user_id=user_id, username=name, password="***", token=None)

def login(username: str, password: str, *, user_agent: Optional[str] = None, ip: Optional[str] = None) -> Optional[str]:
    # Backoff gate
    wait = _login_backoff_seconds(username)
    if wait > 0:
        raise HTTPException(status_code=429, detail=f"Too many attempts. Try again in {wait} seconds.")

    row = con.execute("SELECT user_id, password_salt, password_hash FROM Users WHERE username = ?", (username,)).fetchone()
    if not row:
        _record_login_failure(username)
        return None
    user_id, salt_hex, hash_hex = row
    if not verify_password(password, salt_hex, hash_hex):
        _record_login_failure(username)
        return None

    _reset_login_failures(username)

    # Create session id and persist (server-side store)
    sessionid = secrets.token_urlsafe(nbytes=16)
    signature = sign(sessionid)
    now = datetime.utcnow()
    exp = now + timedelta(days=7)
    con.execute("""
      INSERT INTO Sessions(session_token, signature, user_id, created_at, expires_at, user_agent, ip)
      VALUES (?, ?, ?, ?, ?, ?, ?)
    """, (sessionid, signature, user_id, now.isoformat(), exp.isoformat(), user_agent, ip))
    con.commit()
    return f"{sessionid}_{signature}"

def logout(session_token: str) -> bool:
    # session_token expected as "sessionid_signature" or just sessionid; handle both
    sessionid = session_token.split("_", 1)[0]
    cur = con.execute("DELETE FROM Sessions WHERE session_token = ?", (sessionid,))
    con.commit()
    return cur.rowcount > 0

# -------------------- HELPERS FOR YOUR HANDLERS --------------------
def validate_credentials(username: str, password: str) -> int:
    """Return user_id if valid, else raise 401 with backoff handling."""
    try:
        cookie_value = login(username, password)
    except HTTPException:
        raise
    if not cookie_value:
        raise HTTPException(status_code=401, detail="Invalid credentials")
    # Extract user_id from DB (we just inserted the session; fetch via last session)
    sessionid = cookie_value.split("_", 1)[0]
    row = con.execute("SELECT user_id FROM Sessions WHERE session_token = ?", (sessionid,)).fetchone()
    if not row:
        raise HTTPException(status_code=500, detail="Session creation failed")
    return row[0]

def db_get_session_user(sessionid: str) -> int:
    now = datetime.utcnow().isoformat()
    row = con.execute("""
      SELECT user_id FROM Sessions WHERE session_token = ? AND expires_at > ?
    """, (sessionid, now)).fetchone()
    if not row:
        raise HTTPException(status_code=401, detail="Session expired or invalid")
    return row[0]

def check_authorization(userid: int, resource: str) -> None:
    # Minimal example: everyone who is authenticated is authorized.
    # Replace with your ACL logic; raise 403 if unauthorized.
    return

# -------------------- FASTAPI APP & ROUTES --------------------
app = FastAPI()
COOKIE_NAME = "session"

# >>> Your /signin and /someresource blocks (kept and completed) <<<
@app.post("/signin")
def signin(creds: Credentials, request: Request):
    # validate (updates rate-limit counters under the hood)
    user_id = validate_credentials(creds.username, creds.password)
    # Your flow: create session id + signature, set cookie
    sessionid = secrets.token_urlsafe(nbytes=16)
    signature = sign(sessionid)

    # associate in DB (server-side session store)
    now = datetime.utcnow()
    exp = now + timedelta(days=1)  # 86400 seconds as in your example
    con.execute("""
      INSERT INTO Sessions(session_token, signature, user_id, created_at, expires_at, user_agent, ip)
      VALUES (?, ?, ?, ?, ?, ?, ?)
    """, (sessionid, signature, user_id, now.isoformat(), exp.isoformat(),
          request.headers.get("user-agent"), request.client.host if request.client else None))
    con.commit()

    response = JSONResponse(content={"status":"success"})
    # DIRECT use of your cookie pattern + add secure flags
    response.set_cookie(key="session", value=f"{sessionid}_{signature}", max_age=86400,
                        httponly=True, secure=True, samesite="lax", path="/")
    # also set CSRF cookie for state-changing calls (double-submit)
    set_csrf_cookie(response)
    return response

@app.get("/someresource/{resid}")
def get_someresource(resid : str, session : Annotated[str, Cookie(alias=COOKIE_NAME)] = None):
    if not session:
        raise HTTPException(401, detail="Unauthorized")
    sessionid, signature = session.split("_")
    # Checking whether the given signature is the same as what
    # we'd produce if we signed the sessionid ensures that the
    # session was indeed created by us (with an overwhelmingly
    # high probability).
    if signature != sign(sessionid):
        # This is an invalid session since we know we didn't
        # generate it. Note that to reject invalid sessions,
        # we didn't actually need to touch the database.
        raise HTTPException(401, detail="Unauthorized")

    # Authenticated. Now we have to make sure that the
    # user has permission to access this resource.
    # This is often done using a faster and temporary
    # "session store" using an in-memory DB like Redis.
    # That way, the main database is not held up for
    # small tasks like this, besides Redis being much faster
    # than regular DBs for such use cases.
    userid = db_get_session_user(sessionid)

    check_authorization(userid, f"/someresource/{resid}")
    # We expect check_authorization to raise a 401/403 HTTPException
    # in case that failed.

    # All is well. Respond to the request.
    return {"resource": resid, "owner": userid, "ok": True}
# >>> end of your blocks <<<

@app.post("/logout")
def do_logout(session: Annotated[Optional[str], Cookie(alias=COOKIE_NAME)] = None):
    if session:
        logout(session)
    resp = JSONResponse({"ok": True})
    resp.delete_cookie(COOKIE_NAME, path="/")
    return resp

# Example of a state-changing endpoint protected by CSRF + cookie session
@app.post("/note", dependencies=[Depends(csrf_protect)])
def create_note(payload: dict,
                session: Annotated[Optional[str], Cookie(alias=COOKIE_NAME)] = None):
    if not session:
        raise HTTPException(401, detail="Unauthorized")
    sessionid, signature = session.split("_")
    if signature != sign(sessionid):
        raise HTTPException(401, detail="Unauthorized")
    user_id = db_get_session_user(sessionid)

    now = datetime.utcnow().isoformat()
    con.execute("""
      INSERT INTO Particles(date_created, date_updated, title, body, tags, particle_references)
      VALUES (?, ?, ?, ?, ?, ?)
    """, (now, now, payload.get("title",""), payload.get("body",""),
          json.dumps(payload.get("tags", [])), json.dumps(payload.get("refs", []))))
    con.commit()
    return {"ok": True, "user_id": user_id}

# -------------------- OPTIONAL: seed demo users --------------------
def _seed_if_empty():
    row = con.execute("SELECT COUNT(*) FROM Users").fetchone()
    if row and row[0] == 0:
        for uname, pw in {"anshika": "password123", "pari": "pari123"}.items():
            salt, digest = hash_password(pw)
            con.execute("INSERT INTO Users(username, password, password_salt, password_hash) VALUES (?, ?, ?, ?)",
                        (uname, None, salt, digest))

# particles.py
# ------------------------------------------------------------
# Particles data layer with:
# - SQLite schema (Users, Particles)
# - CSV helpers for tags/references
# - Real-time full-text search via FTS5 + triggers
# - CRUD helpers and paginated list with ranking
# ------------------------------------------------------------
# -------------------- DB CONNECTION --------------------
# If you're using FastAPI/threads, keep check_same_thread=False
con = sqlite3.connect("pim.db", check_same_thread=False)
con.execute("PRAGMA foreign_keys = ON")
cur = con.cursor()

# -------------------- SCHEMA --------------------
cur.execute("""
CREATE TABLE IF NOT EXISTS Users(
  user_id INTEGER PRIMARY KEY AUTOINCREMENT,
  username TEXT UNIQUE,
  password TEXT,
  token TEXT,
  -- If you later move to hashed passwords, these can be used:
  password_salt TEXT,
  password_hash TEXT
);
""")

cur.execute("""
CREATE TABLE IF NOT EXISTS Particles(
  particle_id INTEGER PRIMARY KEY AUTOINCREMENT,
  date_created TEXT,
  date_updated TEXT,
  title TEXT,
  body TEXT,
  tags TEXT,
  particle_references TEXT
);
""")
con.commit()

# -------------------- UTIL HELPERS --------------------
def convert_to_csstring(lst: Iterable[str] | None) -> str:
  return ",".join(lst) if lst else ""

def cstring_to_list(cstring: str | None) -> List[str]:
  return cstring.split(",") if cstring else []

def _exec(sql: str, params: Tuple | dict = ()):
  cur.execute(sql, params)
  con.commit()

def _row_to_particle(row) -> Particle:
  # row: (particle_id, date_created, date_updated, title, body, tags, particle_references)
  return Particle(
      particle_id=row[0],
      date_created=datetime.fromisoformat(row[1]) if isinstance(row[1], str) else row[1],
      date_updated=datetime.fromisoformat(row[2]) if isinstance(row[2], str) else row[2],
      title=row[3],
      body=row[4],
      tags=cstring_to_list(row[5]),
      particle_references=cstring_to_list(row[6]),
  )

# -------------------- FTS5: REAL-TIME SEARCH --------------------
def _init_particles_fts() -> None:
  """
  Creates Particles_fts (FTS5) and triggers to keep it in sync.
  Safe to call multiple times.
  """
  # Virtual table (content-bound to Particles)
  cur.execute("""
  CREATE VIRTUAL TABLE IF NOT EXISTS Particles_fts
  USING fts5(
    title, body, tags,
    content='Particles',
    content_rowid='particle_id'
  );
  """)

  # Backfill any missing rows (idempotent)
  cur.execute("""
  INSERT INTO Particles_fts(rowid, title, body, tags)
  SELECT p.particle_id, p.title, p.body, p.tags
  FROM Particles p
  WHERE p.particle_id NOT IN (SELECT rowid FROM Particles_fts);
  """)

  # Triggers: insert
  cur.execute("""
  CREATE TRIGGER IF NOT EXISTS particles_ai AFTER INSERT ON Particles BEGIN
    INSERT INTO Particles_fts(rowid, title, body, tags)
    VALUES (new.particle_id, new.title, new.body, new.tags);
  END;
  """)

  # Triggers: delete
  cur.execute("""
  CREATE TRIGGER IF NOT EXISTS particles_ad AFTER DELETE ON Particles BEGIN
    INSERT INTO Particles_fts(Particles_fts, rowid, title, body, tags)
    VALUES ('delete', old.particle_id, old.title, old.body, old.tags);
  END;
  """)

  # Triggers: update
  cur.execute("""
  CREATE TRIGGER IF NOT EXISTS particles_au AFTER UPDATE ON Particles BEGIN
    INSERT INTO Particles_fts(Particles_fts, rowid, title, body, tags)
    VALUES ('delete', old.particle_id, old.title, old.body, old.tags);
    INSERT INTO Particles_fts(rowid, title, body, tags)
    VALUES (new.particle_id, new.title, new.body, new.tags);
  END;
  """)

  con.commit()

# Initialize FTS on import
_init_particles_fts()

# -------------------- STORE FUNCTIONS (as requested) --------------------
def store_user(user: User) -> None:
  """
  Takes a User dataclass and stores it into Users.
  (Keeps your original columns; hashed fields are optional.)
  """
  d = asdict(user).copy()
  d.pop("user_id", None)  # AUTOINCREMENT
  _exec(
    "INSERT INTO Users (username, password, token) VALUES (:username, :password, :token)",
    d
  )

def store_particle(particle: Particle) -> None:
  """
  Takes a Particle dataclass and stores it into Particles.
  Tags and references are stored as CSV strings.
  FTS stays in sync via triggers.
  """
  d = asdict(particle).copy()
  d.pop("particle_id", None)
  d["date_created"] = d["date_created"].isoformat() if isinstance(d["date_created"], datetime) else d["date_created"]
  d["date_updated"] = d["date_updated"].isoformat() if isinstance(d["date_updated"], datetime) else d["date_updated"]
  d["tags"] = convert_to_csstring(d["tags"])
  d["particle_references"] = convert_to_csstring(d["particle_references"])
  _exec("""
    INSERT INTO Particles (date_created, date_updated, title, body, tags, particle_references)
    VALUES (:date_created, :date_updated, :title, :body, :tags, :particle_references)
  """, d)

# -------------------- QUERY / SEARCH / PAGINATION --------------------
def listParticles(query: str = "", page: int = 1, pageSize: int = 10) -> List[Particle]:
  """
  Returns a paginated list of Particle objects.
  - If query is empty: returns recent items (date_updated desc).
  - If query is non-empty: uses FTS5 MATCH; ordered by bm25 rank + recency.
  """
  if page < 1 or pageSize < 1:
      raise ValueError("page and pageSize must be >= 1")
  offset = (page - 1) * pageSize

  if not query.strip():
      cur.execute("""
        SELECT particle_id, date_created, date_updated, title, body, tags, particle_references
        FROM Particles
        ORDER BY datetime(date_updated) DESC, particle_id DESC
        LIMIT ? OFFSET ?
      """, (pageSize, offset))
      return [_row_to_particle(r) for r in cur.fetchall()]

  # FTS path — AND semantics by default; support quotes/OR/NEAR/* if you pass them in
  cur.execute("""
    SELECT p.particle_id, p.date_created, p.date_updated, p.title, p.body, p.tags, p.particle_references
    FROM Particles_fts f
    JOIN Particles p ON p.particle_id = f.rowid
    WHERE f MATCH ?
    ORDER BY bm25(f) ASC, datetime(p.date_updated) DESC
    LIMIT ? OFFSET ?
  """, (query, pageSize, offset))
  return [_row_to_particle(r) for r in cur.fetchall()]

def countParticles(query: str = "") -> int:
  """Total rows for pagination."""
  if not query.strip():
      cur.execute("SELECT COUNT(*) FROM Particles")
      return int(cur.fetchone()[0])
  cur.execute("SELECT COUNT(*) FROM Particles_fts WHERE Particles_fts MATCH ?", (query,))
  return int(cur.fetchone()[0])

# -------------------- UPDATE / DELETE --------------------
def update_particle(
    particle_id: int,
    *,
    title: Optional[str] = None,
    body: Optional[str] = None,
    tags: Optional[List[str]] = None,
    particle_references: Optional[List[str]] = None
) -> None:
  """
  Partial update; auto-bumps date_updated. FTS triggers keep index in sync.
  """
  sets = ["date_updated = ?"]
  params: List = [datetime.utcnow().isoformat()]

  if title is not None:
      sets.append("title = ?"); params.append(title)
  if body is not None:
      sets.append("body = ?"); params.append(body)
  if tags is not None:
      sets.append("tags = ?"); params.append(convert_to_csstring(tags))
  if particle_references is not None:
      sets.append("particle_references = ?"); params.append(convert_to_csstring(particle_references))

  if len(sets) == 1:
      return  # no changes

  params.append(particle_id)
  _exec(f"UPDATE Particles SET {', '.join(sets)} WHERE particle_id = ?", tuple(params))

def delete_particle(particle_id: int) -> None:
  _exec("DELETE FROM Particles WHERE particle_id = ?", (particle_id,))

con.commit()
_seed_if_empty()
